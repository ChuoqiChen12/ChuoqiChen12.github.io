<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="pv-proxy-endpoint" content="https://my-blog2-349614.uk.r.appspot.com/query?id=ahFkfm15LWJsb2cyLTM0OTYxNHIVCxIIQXBpUXVlcnkYgICA2LjugwoM"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="Abdominal Medical Images Segmentation with CNN-Transformer Hybrid Model" /><meta name="author" content="Chuoqi Chen" /><meta property="og:locale" content="en" /><meta name="description" content="Welcome to my blog!" /><meta property="og:description" content="Welcome to my blog!" /><link rel="canonical" href="https://chuoqichen12.github.io/posts/image-segementation/" /><meta property="og:url" content="https://chuoqichen12.github.io/posts/image-segementation/" /><meta property="og:site_name" content="CCQ‚Äôs Blog" /><meta property="og:image" content="https://chuoqichen12.github.io/commons/devices-mockup.png" /><meta property="og:image:height" content="500" /><meta property="og:image:width" content="800" /><meta property="og:image:alt" content="Responsive rendering of Chirpy theme on multiple devices." /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-05-06T18:06:00-04:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://chuoqichen12.github.io/commons/devices-mockup.png" /><meta name="twitter:image:alt" content="Responsive rendering of Chirpy theme on multiple devices." /><meta property="twitter:title" content="Abdominal Medical Images Segmentation with CNN-Transformer Hybrid Model" /><meta name="twitter:site" content="@ChuoqiChen" /><meta name="twitter:creator" content="@Chuoqi Chen" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Chuoqi Chen"},"dateModified":"2022-09-19T15:27:54-04:00","datePublished":"2022-05-06T18:06:00-04:00","description":"Welcome to my blog!","headline":"Abdominal Medical Images Segmentation with CNN-Transformer Hybrid Model","image":{"width":800,"height":500,"alt":"Responsive rendering of Chirpy theme on multiple devices.","url":"https://chuoqichen12.github.io/commons/devices-mockup.png","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://chuoqichen12.github.io/posts/image-segementation/"},"url":"https://chuoqichen12.github.io/posts/image-segementation/"}</script><title>Abdominal Medical Images Segmentation with CNN-Transformer Hybrid Model | CCQ's Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="CCQ's Blog"><meta name="application-name" content="CCQ's Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://my-blog2-349614.uk.r.appspot.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://my-blog2-349614.uk.r.appspot.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" /assets/img/ava.JPG " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">CCQ's Blog</a></div><div class="site-subtitle font-italic">Ë™∞Ë¨ÇÊ≤≥Âª£Ôºå‰∏ÄËë¶Êù≠‰πã</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/Fun_In_Life/" class="nav-link"> <i class="fa-fw fas fa-leaf ml-xl-3 mr-xl-3 unloaded"></i> <span>FUN IN LIFE üç≠</span> </a><li class="nav-item"> <a href="/musing_and_quotes/" class="nav-link"> <i class="fa-fw fas fa-sticky-note ml-xl-3 mr-xl-3 unloaded"></i> <span>MUSING AND QUOTES üí≠</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/ChuoqiChen12" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/ChuoqiChen" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['chuoqic','umich.edu'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Abdominal Medical Images Segmentation with CNN-Transformer Hybrid Model</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Abdominal Medical Images Segmentation with CNN-Transformer Hybrid Model</h1><div class="post-meta text-muted"><div> By <em> <a href="https://twitter.com/ChuoqiChen">Chuoqi Chen</a> </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1651874760" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-05-06 </em> </span> <span> Updated <em class="timeago" data-ts="1663615674" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-09-19 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2974 words"> <em>16 min</em> read</span> <span> <em id="pv" class="pageviews"> <i class="fas fa-spinner fa-spin fa-fw"></i> </em> views </span></div></div></div><div class="post-content"><p><strong>EECS545 Coursework Project</strong></p><p><strong>Duration</strong>: <em>02/2022-05/2022</em></p><p><strong>Teammates</strong>: Haowen Chen, Yuang Lu, Yuqi Yan, Ying Yuan</p><hr /><h2 id="introduction"><span class="mr-2">Introduction</span><a href="#introduction" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p align="justify"> During Diagnosis, treatment and surgery with computer aid, medical image segmentation plays a crucial part in identifying pixels of anatomical objects in medical images. However, it is a time-consuming and heavy work to conduct segmentation on images one by one by doctors. The primary challenges for segmentation mainly come from three aspects: (1) Complex boundary interactions, (2) Large appearance variation, (3) Low tissue contrast. These challenges. With the advance of deep learning, it is necessary to conduct new techniques to help medical image segmentation. <a href="#reference"> [1] </a> <a id="ref1"></a></p><p align="justify"> In this project, we focused on the abdominal multi-organs segmentation problem. We proposed two novel neural network structures to solve two main problems: fragmental objects edges and failure to detect small, direction-dependent objects. The performance of medical image segmentation is expected to be enhanced by fusing Self Attention mechanism and convolutional neural network.</p><h2 id="related-work"><span class="mr-2">Related Work</span><a href="#related-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p align="justify"> In this section, recent work for medical image segmentation will be gone through. To achieve pixel-wise classification, researchers proposed the encoder-decoder network structure. UNet is one of the most outstanding works among them. Based on the structure, novel techniques have been introduced into UNet. <a href="#reference"> [2] </a> <a id="ref2"></a></p><h3 id="resunet"><span class="mr-2">ResUNet</span><a href="#resunet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> Residual UNet(ResUNet)<a href="#reference"> [3] </a> <a id="ref3"></a> is inspired by ResNet.<a href="#reference"> [4] </a> <a id="ref4"></a> Previous results showed that with the number of network layers increased, the feature identities and performance will be lost. It is caused by vanishing gradients in deep networks. By concatenating features, the skip connection before downsampling or upsampling in ResUNet helps preserve feature maps.</p><p align="justify"> According to our results, ResUNet is capable of detecting all 13 organs in the Synapse dataset including small and directional objects such as the left and right adrenal gland. However, all other transformer-based models failed to do so. It can also perform better in segmenting other small organs such as portal vein and splenic vein. It is believed that the convolutional layer helps the model to detect and segmentate these small objects.</p><h3 id="transunet"><span class="mr-2">TransUNet</span><a href="#transunet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> TranUNet is a model based on Transformers and UNet, leveraging both detailed high-resolution spatial information from CNN features and the global context encoded by Transformers. Transformer serves as a strong encoder, providing an attentive feature sequence. According to former results, this architecture achieves superior performance over any other method on medical image segmentation. Therefore, TransUNet works as the baseline model in this project and implements a novel model.<a href="#reference"> [5] </a> <a id="ref5"></a></p><h3 id="swin-unet"><span class="mr-2">Swin-UNet</span><a href="#swin-unet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> Based on the outstanding performance in Trans-UNet, optimization of Transformer might be opt for image segmentation. Swin-UNet could be one among them. Different from all silices connected together in Trans-UNet, the limited slices are connected with the neighboring slices in Swin-UNet. When in the higher layer, the output from the neighbor transformer will be the input of the next layer. By this way, more details from the images could be trained and neighboring information could affect each other. <a href="#reference"> [6] </a> <a id="ref6"></a></p><p align="justify"> Though Swin-UNet has higher DICE score over TransUNet overall, which means that it could better classify each pixel in the right class, the results showed that the segmented objects trained by Swin-UNet are fragmental and have rougher edges compared to TransUNet.</p><h3 id="utnet"><span class="mr-2">UTNet</span><a href="#utnet" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> Although transformer variants perform well in vision tasks, pixels packed in the same input image patch do not contain enough spatial information, leading to weaker inductive bias compared with traditional CNN. Therefore, vision transformer-based models require a large image set to gain enough prior knowledge in the image domain. However, a major characteristic of medical image segmentation tasks is that they usually cannot provide such a big dataset, using pretrained weight on other large image datasets then becomes a widely adopted solution.</p><p align="justify"> UTNet, unlike other models, employs another method. Common vision transformers and their variants pack pixels into patches and flattening them into vectors while UTNet preserves the first few encoding layers as convolutional layers, the pixel-level spatial relations thus can be retained.<a href="#reference"> [7] </a> <a id="ref7"></a></p><h3 id="unetr"><span class="mr-2">UNETR</span><a href="#unetr" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> UNETR proposed a novel transformer-based model for volumetric medical image segmentation. Instead of extracting features with Transformers at the bottleneck of U-Net, UNETR has its skip-connected decoder that combines representation from multiple intermediate Transformers layers. Deconvolutional layers are applied to these intermediate representations to increase the resolution. Such architecture helps the network capture global contextual representation at multiple scales and increase the capability for learning long-range dependencies.<a href="#reference"> [8] </a> <a id="ref8"></a></p><h3 id="transfuse"><span class="mr-2">TransFuse</span><a href="#transfuse" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> In order to improve the efficiency of modeling global contexts and preserve low-level features, TransFuse uses a parallel approach to merge Transformers and CNNs. This allows the efficient collection of both global dependencies and low-level spatial features in a significantly shallower manner. The BiFusion module is a novel fusion technique that efficiently fuses the multi-level features from both branches. According to the research, TransFuse obtains state-of-the-art results on both 2D and 3D medical picture sets with significantly fewer parameters and significantly faster inference.<a href="#reference"> [9] </a> <a id="ref9"></a></p><h2 id="proposed-method"><span class="mr-2">Proposed Method</span><a href="#proposed-method" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p align="justify"> In our novel models, Swin-Transformer and UTNets act as encodes to learn sequence representations of the input and further enhance the efficiency in capturing the global information. The overall diagram of the proposed model is shown in Fig. 1.</p><p align="center"> <img data-src="/assets/img/post/2022-05-06-image-segementation/pipeline2.png" data-proofer-ignore></p><p align="center"> Fig 1. - Pipeline</p><p>This model consists of two parallel branches that process information differently</p><h4 id="1-cnn-branch"><span class="mr-2">1. CNN Branch</span><a href="#1-cnn-branch" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><p align="justify"> It gradually increases the receptive field and encodes the feature from local to global. With the Transformer branch to obtain global context information instead, CNN branch could be shallower, and the proposed model could be also retaining richer local information. The output of the 3rd block ($\textbf{l}^1\in \mathbb{R}^{\frac{H}{16} \times \frac{W}{16}\times C_1 }$), 2nd block ($\textbf{l}^2\in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8}\times C_2 }$), and 1st block ($\textbf{l}^3\in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4}\times C_3 }$)) are fused with the results from Transformer. We propose different models with CNN variants.</p><h4 id="2-transformer-branch"><span class="mr-2">2. Transformer Branch</span><a href="#2-transformer-branch" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><p align="justify"> It starts with global self-attention and recovers the local details at the end. The input image $\textbf{x} \in \mathbb{R}^{H\times W\times 3}$ is first evenly divided into $N =\frac{H}{16} \times \frac{W}{16}$ patches. Then the consecutive $3 \times 3 \times 3$ upsampling-convolution layers is used to obtain $\textbf{z}^1\in \mathbb{R}^{\frac{H}{16} \times \frac{W}{16}\times D_1 }$, $\textbf{z}^2\in \mathbb{R}^{\frac{H}{8} \times \frac{W}{8}\times D_2 }$, and $\textbf{z}^3\in \mathbb{R}^{\frac{H}{4} \times \frac{W}{4}\times D_3 }$. We propose different models with transformer variants.</p><h4 id="3-bifusion-module"><span class="mr-2">3. BiFusion Module</span><a href="#3-bifusion-module" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><p>It fuses the encoded features of the same resolution extracted from CNN and Transformer branch. The fused feature $\textbf{b}^i, i = 1,2,3$ is obtained by the following operations:</p><p align="center"> $\hat{\textbf{z}}^i = \text{ChannelAttn}({\textbf{z}}^i)$</p><p align="center"> $\hat{\textbf{l}}^i = \text{SpatialAttn}({\textbf{l}}^i)$</p><p align="center"> $\hat{\textbf{c}}^i = \text{Conv}(\textbf{z}^i\textbf{W}_1^i\odot\textbf{l}^i\textbf{W}_2^i)$</p><p align="center"> $\hat{\textbf{b}}^i = \text{Residual}([\hat{\textbf{c}}^i,\hat{\textbf{z}}^i,\hat{\textbf{l}}^i])$</p><p align="justify"> where $\textbf{W}_1^i \in \mathbb{R}^{D_i\times L_i}$, $\textbf{W}_2^i \in \mathbb{R}^{C_i\times L_i}$, $|\odot|$ is the Hadamard product and Conv is a $3\times3$ convolution layer. The channel attention is implemented according to the SE-Block proposed by to facilitate global information from the Transformer branch.<a href="#reference"> [10] </a> <a id="ref10"></a> The spatial attention is adopted from CBAM block as spatial filters to enhance local details.<a href="#reference"> [11] </a> <a id="ref11"></a></p><h4 id="4-attention-gated-skip-connection"><span class="mr-2">4. Attention-gated skip-connection</span><a href="#4-attention-gated-skip-connection" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4></h4><p align="justify"> It combines fused feature maps and generates the segmentation.<a href="#reference"> [12] </a><a id="ref12"></a>We obtain $\hat{\textbf{f}}^{i+1} = \text{Conv}([\text{Up}(\hat{\textbf{f}}^{i}), \text{AG}({\textbf{f}}^{i+1}, \text{Up}(\hat{\textbf{f}}^{i}))])$ and $\textbf{f}^{1}=\hat{\textbf{f}}^{1}$.</p><p align="justify"> With this branch-in-parallel approach, firstly, global information can be captured and sensitivity could be preserved on low-level context without building deep nets; secondly, the fused representation would be powerful and compact with the BiFusion module by utilizing the different features of CNN and Transformer simultaneously.</p><h3 id="swin-fuse"><span class="mr-2">Swin-Fuse</span><a href="#swin-fuse" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> In order to solve the problem of fragmental edges of Swin-UNet, the Swin-Transformer is optimized with the BiFusion module. By funding the encoder of ResNet and encoder od Swin-Transformer together, low level details captured by the CNN branch and the global information captured by the Transformer branch can be combined together by the BiFusion module and then sent to the decode/ The hybrid model therefore can preserve the strength of high prediction accuracy of Swin-UNet and further improve the model's ability of restoring edges of organs with the help of the captured low-level details.</p><p align="justify"> Swin-Fuse is trained with ADA&lt; optimizer with learning rate set to 0.001 Beta1 and Beta 2 are set to 0.5 and 0.999 correspondingly. The loss of the output is the weighted sum of the cross entropy and DICE score with weights of 0.4 and 0.6. The cross entropy and DICE loss are calculated based on the intermediate result and final result, consistent with the method introduced by the TransFuse.</p><h3 id="ut-fuse"><span class="mr-2">UT-Fuse</span><a href="#ut-fuse" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> In practice, all transformer-based models such as Trans-UNet, Swin-UNet, UTNet and hybrid models like TransFuse and Swin-Fuse failed to segment left adrenal gland and right adrenal gland. After analyzing the characteristics of these two organs, it is found that they are small and direction-dependent organs, that they are located almost in the same slice of the 3D CT scan, and only differentiating in the direction (left and right oriented) and the relative location in the 2D image. In order to successfully segment these two organs, the model's ability of classifying small organs based on the neighboring organ and also tell the direction of organs is needed to enhance.</p><p align="justify"> Therefore, we proposed UT-Fuse, fusing ResNet and UTNet with the BiFusion module. The model is capable of capturing pixel-level details from the input image from both the transformer branch and the CNN branch. The transformer branch will also process the location information with global self-attention. Since both branches have an intermediate convolutional layer, neighboring organs have stronger connections and the model can detect direction dependent organs based on the relative location with other organs.</p><p align="justify"> Training method and parameters of UT-Fuse is consistent with that of Swin-Fuse.</p><h2 id="implementation"><span class="mr-2">Implementation</span><a href="#implementation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p align="justify"> Synapse multi-organ image segmentation dataset acts as the data in this project, that was originally used for the MICCAI 2015 Multi-Atlas Abdomen Labeling Challenge.<a href="#reference"> [13] </a> <a id="ref13"></a> 50 abdomen CT scans are included in the dataset. To align with previous works, 30 out of 50 scans are used for this project. These CT scans range from $512 \times 512 \times 85$ to $512 \times 512 \times 198$ in pixels with resolution varying from $0.54 \times 0.54$ $mm^2$ to $0.98 \times 0.98$ $mm^2$. 13 different organs, including spleen, stomach, liver, etc. are labeled pixel by pixel.</p><p align="justify"> 3D CT scan is cut into $512 \times 512$ pixel 2D images, and splited 12 out of 30 scans as testing images, which is consistent to preprocessing techniques mentioned in SwinUNet and TranUNet. Models such as TranUNet and SwinUNet use pretrained weight on large image dataset while novel architectural models like UTNet use a unique down-sampling method that requires specific image dimension. Therefore, the image is cropped to [224,224] or [256, 256] in dimension corresponding to the model requirements. In order to improve the robustness of the model, data augmentation methods such as random flip and random rotation have been applied.</p><h2 id="evaluation"><span class="mr-2">Evaluation</span><a href="#evaluation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p align="justify"> Though many transformer-based models reported better performance compared to traditional Res-UNet, only models of comparable sizes are used for evaluation. For example, Res-UNet based on ResNet-34 which has 63.5 million parameters and TransUNet based on vit-base-patch 16-224 which has 86 million parameters are used as reference models. The implemented hybrid model uses the pre-trained weight of ResNet-34 and Tiny Swin-Transformer (28 million parameters).</p><p align="justify"> For medical images segmentation, there are two types of errors related to segmentation accuracy, namely the delineation of the boundary (contour) and the size (volume of the segmented object). Two metrics for these two types of errors are selected based on literature review.<a href="#reference"> [14] </a> <a id="ref14"></a></p><p><strong>Dice Coefficient</strong>, also called the overlap index, is the most used metric in validating medical image segmentation, which is defined as</p><p align="center"> $DICE = \frac{2TP}{2TP + FP + FN}$</p><p><strong>Hausdorff Distance(HD)</strong> measures the distance between crisp volumes. Given two point sets $A$ and $B$, HD is defined by</p><p align="center"> $HD(A, B) = \max(h(A, B), h(B,A))$</p><p>where $h(A, B)$ is called the directed Hausdorff distance and given by</p><p align="center"> $h(A, B) = \max_{a\in A}\min_{b\in B} ||a-b||$</p><p>where $||\cdot||$ is a norm(normally Euclidean distance). Note that HD is sensitive to outliers and the $q$-th quantile of distances may be used instead of the maximum, so that possible outliers are excluded.</p><h2 id="result-analysis"><span class="mr-2">Result Analysis</span><a href="#result-analysis" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p align="justify"> In this part, the average Dice Coefficient and average Hausdorff Distance(HD) on 13 organs will be reported. The organs included spleen, right kidney, left kidney, gallbladder, esophagus, liver, stomach, aorta, inferior vena cava, portal vein and splenic vein, pancreas, right adrenal gland, and left adrenal gland. They are splitted of 18 training cases (2212 axial slices) and 12 cases for validation. Table 1 and Table 2 showed the results comparison.</p><div class="table-wrapper"><table><thead><tr><th>DICE Score<th>ResUNet<th>TransUNet<th>SwinUNet<th>UTNet<th>TransFuse<th>Swin-Fuse<th>UT-Fuse<tbody><tr><td>spleen<td>81.43%<td>83.43%<td>87.49%<td>86.46%<td>86.07%<td>89.45%<td>87.10%<tr><td>right kidney<td>71.77%<td>69.89%<td>77.00%<td>83.03%<td>79.07%<td>79.81%<td>72.90%<tr><td>left kidney<td>80.12%<td>74.83%<td>81.15%<td>86.12%<td>79.08%<td>86.64%<td>82.11%<tr><td>gallbladder<td>61.17%<td>45.57%<td>60.04%<td>70.97%<td>26.68%<td>63.96%<td>58.67%<tr><td>esophagus<td>71.53%<td>54.54%<td>67.64%<td>73.76%<td>69.90%<td>66.48%<td>66.12%<tr><td>liver<td>93.71%<td>91.51%<td>93.42%<td>95.42%<td>93.79%<td>93.21%<td>93.92%<tr><td>stomach<td>73.92%<td>72.43%<td>72.46%<td>75.50%<td>71.45%<td>73.36%<td>80.70%<tr><td>aorta<td>87.39%<td>70.97%<td>83.18%<td>89.99%<td>85.94%<td>86.34%<td>87.34%<tr><td>inferior vena cava<td>74.47%<td>59.69%<td>70.57%<td>81.33%<td>76.52%<td>77.59%<td>69.65%<tr><td>portal vein and splenic vein<td>66.32%<td>39.60%<td>54.13%<td>66.69%<td>55.67%<td>57.43%<td>62.08%<tr><td>pancreas<td>61.22%<td>45.00%<td>54.81%<td>62.34%<td>62.30%<td>56.49%<td>61.72%<tr><td>right adrenal gland<td>59.26%<td>0.00%<td>0.00%<td>0.00%<td>0.00%<td>0.00%<td>45.91%<tr><td>left adrenal gland<td>58.21%<td>0.00%<td>0.00%<td>0.00%<td>0.00%<td>0.00%<td>50.33%<tr><td>Mean<td>72.35%<td>54.42%<td>61.68%<td>67.05%<td>60.50%<td>63.90%<td>70.66%</table></div><p align="center"> Table 1: Mean Dice by organs</p><div class="table-wrapper"><table><thead><tr><th>DICE Score<th>ResUNet<th>TransUNet<th>SwinUNet<th>UTNet<th>TransFuse<th>Swin-Fuse<th>UT-Fuse<tbody><tr><td>spleen<td>68.37<td>31.07<td>29.63<td>35.77<td>34.35<td>33.60<td>8.78<tr><td>right kidney<td>74.55<td>62.91<td>31.54<td>54.66<td>41.48<td>19.52<td>39.99<tr><td>left kidney<td>39.32<td>40.32<td>47.11<td>35.62<td>22.19<td>16.18<td>39.22<tr><td>gallbladder<td>24.36<td>38.71<td>29.26<td>10.95<td>10.21<td>10.85<td>14.52<tr><td>esophagus<td>6.52<td>7.94<td>5.93<td>5.14<td>4.61<td>8.78<td>6.11<tr><td>liver<td>26.54<td>25.21<td>21.05<td>12.77<td>14.70<td>27.26<td>20.18<tr><td>stomach<td>23.10<td>17.44<td>18.28<td>17.64<td>25.85<td>16.32<td>11.90<tr><td>aorta<td>15.49<td>11.59<td>10.69<td>4.27<td>10.96<td>11.90<td>6.35<tr><td>inferior vena cava<td>8.30<td>18.33<td>12.89<td>5.43<td>9.28<td>6.16<td>8.86<tr><td>portal vein and splenic vein<td>15.50<td>32.08<td>29.13<td>17.45<td>16.72<td>29.02<td>23.51<tr><td>pancreas<td>10.63<td>20.35<td>15.48<td>12.62<td>12.45<td>20.80<td>11.13<tr><td>right adrenal gland<td>5.31<td>0.00<td>0.00<td>0.00<td>0.00<td>0.00<td>7.55<tr><td>left adrenal gland<td>5.94<td>0.00<td>0.00<td>0.00<td>0.00<td>0.00<td>6.40<tr><td>Mean<td>24.92<td>23.54<td>22.82<td>19.30<td>18.44<td>18.22<td>15.73</table></div><p align="center"> Table 2: Mean hd95 by organs</p><p align="center"> <img data-src="/assets/img/post/2022-05-06-image-segementation/seg_result.png" data-proofer-ignore></p><p align="center"> Fig 2. - Segmentation result of Synapse dataset.</p><p align="center"> <img data-src="/assets/img/post/2022-05-06-image-segementation/zoom.png" data-proofer-ignore></p><p align="center"> Fig 3. - Local segmentation result for small objects</p><h3 id="swin-fuse-1"><span class="mr-2">Swin-Fuse</span><a href="#swin-fuse-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> With the help of BiFusion module and CNN-branch, Swin-Fuse has better performance in capturing the local interactions with neighbouring pixels and therefore it can restore the edge of detected objects better. According to our result, the classified organs from Swin-Fuse could be properly shaped, while edged from Swin-UNet is fragmental, as shown in Fig.2. Comparing mean DICE score and mean HD95 of Swin-Fuse(63.90%, 18.22) and TransUNet(54.42%, 23.54), Swin-UNet has a better performance.</p><h3 id="ut-fuse-1"><span class="mr-2">UT-Fuse</span><a href="#ut-fuse-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p align="justify"> UT-Fuse which is fused by hybrid UTNet and ResNet-34 has a better DICE score compared to other Transformer-based models. With a margin of 3.61\% higher than UTNet (67.05\%) and 16.24\% higher than the original TransUNet. The HD95 of UT-Fuse is also better than other models. The mean DICE score of UT-Fuse is 15.73, beating the second place, Swin-Fuse, by 18.22. UT-Net can also classify and segment small and direction dependent objects such as right and left adrenal gland which all other Transformer-based models failed to segment, with DICE score of 45.91\% and 50.33\%, and HD95 of 7.55 and 6.40. Though UT-Fuse has a lower DICE score than Res-UNet (70.66\% compared to 72.35\%), the HD95 of UT-Fuse (15.73) is much better than that of Res-UNet (24.92).</p><p align="justify"> The performance of UT-Fuse is generally better than almost all other models, however, it is inferior to UTNet in several classes. An interesting observation is that both Res-UNet and UTNet have higher DICE scores for gallbladder than UT-Fuse even though UT-Fuse is composed of these two models. This problem is possibly caused by the unclear boundary of gallbladder with other organs, when the BiFusion module fusing two separate models, information captured by lower layers are mixed up, which makes the model harder to separate Gallbladder and other organs.</p><h2 id="reference"><span class="mr-2">Reference</span><a href="#reference" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p><a href="#ref1">[1]</a> Sihang Zhou, Dong Nie, Ehsan Adeli, Jianping Yin, Jun Lian, and Dinggang Shen. 2019. High-resolution encoder-decoder networks for low-contrast medical image segmentation. IEEE Transactions on Image Processing, 29:461-475.</p><p><a href="#ref2">[2]</a> Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234-241. Springer.</p><p><a href="#ref3">[3]</a> Zhengxin Zhang, Qingjie Liu, and Yunhong Wang. 2018. Road extraction by deep residual u-net. IEEE Geoscience and Remote Sensing Letters, 15(5):749-753.</p><p><a href="#ref4">[4]</a> Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778.</p><p><a href="#ref5">[5]</a> Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. 2021. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv:2102.04306.</p><p><a href="#ref6">[6]</a> Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian, and Manning Wang. 2021. Swin-unet: Unet-like pure transformer for medical image segmentation.</p><p><a href="#ref7">[7]</a> Yunhe Gao, Mu Zhou, and Dimitris N Metaxas. 2021. Utnet: a hybrid transformer architecture for medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 61-71. Springer.</p><p><a href="#ref8">[8]</a> Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger Roth, and Daguang Xu. 2021. Unetr: Transformers for 3d medical image segmentation.</p><p><a href="#ref9">[9]</a> Yundong Zhang, Huiye Liu, and Qiang Hu. 2021. Transfuse: Fusing transformers and cnns for medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 14-24. Springer.</p><p><a href="#ref10">[10]</a> Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-andexcitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132-7141.</p><p><a href="#ref11">[11]</a> Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. 2018. Cbam: Convolutional block attention module. In Proceedings of the European conference on computer vision (ECCV), pages 3-19.</p><p><a href="#ref12">[12]</a> Jo Schlemper, Ozan Oktay, Michiel Schaap, Mattias Heinrich, Bernhard Kainz, Ben Glocker, and Daniel Rueckert. 2019. Attention gated networks: Learning to leverage salient regions in medical images. Medical image analysis, 53:197-207.</p><p><a href="#ref13">[13]</a> info@sagebase.org Sage Bionetworks. Sage bionetworks.</p><p><a href="#ref14">[14]</a> Abdel Aziz Taha and Allan Hanbury. 2015. Metrics for evaluating 3d medical image segmentation: analysis, selection, and tool. BMC medical imaging, 15(1):1-28.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/projects/'>Projects</a>, <a href='/categories/machine-learning/'>Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >machine learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Abdominal+Medical+Images+Segmentation+with+CNN-Transformer+Hybrid+Model+-+CCQ%27s+Blog&url=https%3A%2F%2Fchuoqichen12.github.io%2Fposts%2Fimage-segementation%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Abdominal+Medical+Images+Segmentation+with+CNN-Transformer+Hybrid+Model+-+CCQ%27s+Blog&u=https%3A%2F%2Fchuoqichen12.github.io%2Fposts%2Fimage-segementation%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fchuoqichen12.github.io%2Fposts%2Fimage-segementation%2F&text=Abdominal+Medical+Images+Segmentation+with+CNN-Transformer+Hybrid+Model+-+CCQ%27s+Blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Changelog/">Blog Changelog</a><li><a href="/posts/Book-Notes-Microsystem-Design/">Microsystem Design -- Part 1 Getting Started</a><li><a href="/posts/WL-devices/">Miniature, High-sensitivity, Flexible Devices for Transepidermal Water Loss</a><li><a href="/posts/RISC/">RISC Processor Circuit Design and Physical Design</a><li><a href="/posts/injet/">3D Printing for Electrical Connections in Hybrid System</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/fabrication/">fabrication</a> <a class="post-tag" href="/tags/circuits/">circuits</a> <a class="post-tag" href="/tags/mems/">mems</a> <a class="post-tag" href="/tags/notes/">notes</a> <a class="post-tag" href="/tags/2d-material/">2d material</a> <a class="post-tag" href="/tags/biomedical-device/">biomedical device</a> <a class="post-tag" href="/tags/blogs/">blogs</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/neuroscience/">neuroscience</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/TIA/"><div class="card-body"> <em class="timeago small" data-ts="1643292000" > 2022-01-27 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Wide-band High Gain Differential Transimpedance Amplifier</h3><div class="text-muted small"><p> EECS413 Coursework Project Duration: 11/2021-01/2021 Teammates: Haowen Chen, Zisu Wang Introduction An optical fiber operates at 5 Gbps. This project aims to design a CMOS wideband Transi...</p></div></div></a></div><div class="card"> <a href="/posts/RISC/"><div class="card-body"> <em class="timeago small" data-ts="1643464800" > 2022-01-29 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>RISC Processor Circuit Design and Physical Design</h3><div class="text-muted small"><p> EECS427 Coursework Project Duration: 09/2021-01/2021 Teammates: Yuanqi Guo, Sierra Wang, Ziyu Wang, Xiangdong Wei, Zhengqi Xu Introduction Reduced instruction set computer(RISC) is microsys...</p></div></div></a></div><div class="card"> <a href="/posts/stage/"><div class="card-body"> <em class="timeago small" data-ts="1651183320" > 2022-04-28 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>MicroBeetles:Thermally-Actuated 1-DOF Translational Nano-stage with Closed-loop Positioning</h3><div class="text-muted small"><p> EECS425 Coursework Project Duration: 02/2022-05/2022 Teammates: Daniel Garan, Zhenzhen Gu, Meng-Lin Hseih, Yan Yang Unfinished Typography Introduction Precise positioning of mechanica...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/stage/" class="btn btn-outline-primary" prompt="Older"><p>MicroBeetles:Thermally-Actuated 1-DOF Translational Nano-stage with Closed-loop Positioning</p></a> <a href="/posts/Book-Notes-Microsystem-Design/" class="btn btn-outline-primary" prompt="Newer"><p>Microsystem Design -- Part 1 Getting Started</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "ChuoqiChen12/ChuoqiChen12.github.io", "data-repo-id": "R_kgDOG9Pk0g", "data-category": "Announcements", "data-category-id": "DIC_kwDOG9Pk0s4CPIm2", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "en", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> ¬© 2023 <a href="https://twitter.com/ChuoqiChen">Chuoqi Chen</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/fabrication/">fabrication</a> <a class="post-tag" href="/tags/circuits/">circuits</a> <a class="post-tag" href="/tags/mems/">mems</a> <a class="post-tag" href="/tags/notes/">notes</a> <a class="post-tag" href="/tags/2d-material/">2d material</a> <a class="post-tag" href="/tags/biomedical-device/">biomedical device</a> <a class="post-tag" href="/tags/blogs/">blogs</a> <a class="post-tag" href="/tags/machine-learning/">machine learning</a> <a class="post-tag" href="/tags/neuroscience/">neuroscience</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG ‚Ä∫ <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script async src="https://cdn.jsdelivr.net/npm/countup.js@1.9.3/dist/countUp.min.js"></script> <script defer src="/assets/js/dist/pvreport.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=UA-227243083-1"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'UA-227243083-1'); }); </script>
